{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine Learning Basic Principles 2018 - Data Analysis Project Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*All the text in italics is instructions for filling the template - remove when writing the project report!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Title* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Title should be concise and informative, describes the approach to solve the problem. Some good titles from previous years:*\n",
    "\n",
    "*- Comparing extreme learning machines and naive bayes’ classifier in spam detection*\n",
    "\n",
    "*- Using linear discriminant analysis in spam detection*\n",
    "\n",
    "*Some not-so-good titles:*\n",
    "\n",
    "*- Bayesian spam filtering with extras*\n",
    "\n",
    "*- Two-component classifier for spam detection*\n",
    "\n",
    "*- CS-E3210 Term Project, final report*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Precise summary of the whole report, previews the contents and results. Must be a single paragraph between 100 and 200 words.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Background, problem statement, motivation, many references, description of\n",
    "contents. Introduces the reader to the topic and the broad context within which your\n",
    "research/project fits*\n",
    "\n",
    "*- What do you hope to learn from the project?*\n",
    "*- What question is being addressed?*\n",
    "*- Why is this task important? (motivation)*\n",
    "\n",
    "*Keep it short (half to 1 page).*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Briefly describe data (class distribution, dimensionality) and how will it affect\n",
    "classification. Visualize the data. Don’t focus too much on the meaning of the features,\n",
    "unless you want to.*\n",
    "\n",
    "*- Include histograms showing class distribution.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2181, 264)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "test_data = pd.read_csv(\"./test_data.csv\").values\n",
    "train_data = pd.read_csv(\"./train_data.csv\").values\n",
    "train_labels = pd.read_csv(\"./train_labels.csv\").values\n",
    "\n",
    "# Use every second item of train_data as confirmation data set and every second as training set\n",
    "scaler.fit(train_data[::2])\n",
    "confirm_set = scaler.transform(train_data[::2])\n",
    "confirm_labels = np.ravel(train_labels[::2])\n",
    "train_set = scaler.transform(train_data[1::2])\n",
    "train_labels = np.ravel(train_labels[1::2])\n",
    "\n",
    "\n",
    "classes = [\n",
    "    \"Pop_Rock\",\n",
    "    \"Electronic\",\n",
    "    \"Rap\",\n",
    "    \"Jazz\",\n",
    "    \"Latin\",\n",
    "    \"RnB\",\n",
    "    \"International\",\n",
    "    \"Country\",\n",
    "    \"Reggae\",\n",
    "    \"Blues\"\n",
    "]\n",
    "unique, counts = np.unique(train_labels, return_counts=True)\n",
    "print(train_set.shape)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "hist = ax.bar(classes, counts)\n",
    "ax.set_xlabel(\"Class\")\n",
    "ax.set_ylabel(\"n of samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and cleanup\n",
    "\n",
    "# CLF.fit Takes to parameters data X shape(n_samples, n_features) and labels Y shape(n_samples\n",
    "def train_clf_with_params(alpha, hidden_layer_sizes, random_state, X, Y):\n",
    "    clf = MLPClassifier(solver='lbfgs', alpha=alpha, hidden_layer_sizes=hidden_layer_sizes, random_state=random_state, shuffle=True)\n",
    "    clf.fit(X, Y)\n",
    "    return clf\n",
    "\n",
    "def test_clf(clf, X, Y_true):\n",
    "    Y_pred = clf.predict(X)\n",
    "    score = clf.score(X, Y_true)\n",
    "    Y_pred_proba = clf.predict_proba(X)\n",
    "    print(\"Score: \" + str(score))\n",
    "    loss = log_loss(Y_true, Y_pred_proba)\n",
    "    print(\"Log Loss: \" + str(loss))\n",
    "    return (score, loss)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.579092159559835\n",
      "Log Loss: 7.19328097769484\n",
      "Score: 0.5933058230169647\n",
      "Log Loss: 7.263376395274322\n",
      "Score: 0.5818431911966988\n",
      "Log Loss: 7.519201554388124\n",
      "Score: 0.5653370013755158\n",
      "Log Loss: 9.424112636867326\n",
      "Score: 0.5653370013755158\n",
      "Log Loss: 8.091296125826172\n",
      "Score: 0.5887207702888583\n",
      "Log Loss: 7.098345106874916\n",
      "Score: 0.5795506648326456\n",
      "Log Loss: 7.450960074094607\n",
      "Score: 0.5873452544704264\n",
      "Log Loss: 7.089922211681097\n",
      "Score: 0.5332416322787712\n",
      "Log Loss: 11.846310177108904\n",
      "Score: 0.584135717560752\n",
      "Log Loss: 7.824602175887885\n",
      "Score: 0.5813846859238881\n",
      "Log Loss: 7.4409634275293035\n",
      "Score: 0.5708390646492435\n",
      "Log Loss: 7.462631380013403\n",
      "Score: 0.584135717560752\n",
      "Log Loss: 7.271207791951658\n",
      "Score: 0.5781751490142136\n",
      "Log Loss: 7.688341615248855\n",
      "Score: 0.5855112333791839\n",
      "Log Loss: 7.315099885269148\n",
      "Score: 0.5795506648326456\n",
      "Log Loss: 7.6298425292833\n",
      "Score: 0.5891792755616689\n",
      "Log Loss: 6.08436713155127\n",
      "Score: 0.5919303071985328\n",
      "Log Loss: 7.33643379919162\n",
      "Score: 0.5868867491976157\n",
      "Log Loss: 7.6020623674540815\n",
      "Score: 0.5873452544704264\n",
      "Log Loss: 6.471161647853768\n",
      "Score: 0.5813846859238881\n",
      "Log Loss: 6.945774865927389\n",
      "Score: 0.5717560751948647\n",
      "Log Loss: 8.189782075376675\n",
      "Score: 0.5300320953690968\n",
      "Log Loss: 4.804973774247122\n",
      "Score: 0.5786336542870243\n",
      "Log Loss: 6.644016751315024\n",
      "Score: 0.5914718019257221\n",
      "Log Loss: 6.702806930871229\n",
      "Score: 0.5589179275561669\n",
      "Log Loss: 10.085135039146767\n",
      "Score: 0.5772581384685924\n",
      "Log Loss: 8.145719439970433\n",
      "Score: 0.6070609812012838\n",
      "Log Loss: 6.37356425305794\n",
      "Score: 0.5749656121045392\n",
      "Log Loss: 9.261673958387428\n",
      "Score: 0.5483723062815222\n",
      "Log Loss: 12.322192712208231\n",
      "Score: 0.5786336542870243\n",
      "Log Loss: 6.644016751315024\n",
      "Score: 0.5749656121045392\n",
      "Log Loss: 7.618686277847016\n",
      "Score: 0.5153599266391563\n",
      "Log Loss: 7.053917095524399\n",
      "Score: 0.5749656121045392\n",
      "Log Loss: 7.383989425953883\n",
      "Score: 0.5254470426409904\n",
      "Log Loss: 5.530201261436458\n",
      "Score: 0.5859697386519945\n",
      "Log Loss: 6.894898960960218\n",
      "Score: 0.584135717560752\n",
      "Log Loss: 7.824602175887885\n",
      "Score: 0.569005043558001\n",
      "Log Loss: 8.789553577179081\n",
      "Score: 0.5809261806510775\n",
      "Log Loss: 6.960894116881079\n",
      "Score: 0.5635029802842733\n",
      "Log Loss: 10.497512792402302\n",
      "Score: 0.5882622650160477\n",
      "Log Loss: 6.390575811026361\n",
      "Score: 0.5758826226501604\n",
      "Log Loss: 8.19337346000522\n",
      "Score: 0.5859697386519945\n",
      "Log Loss: 8.12238038723485\n",
      "Score: 0.5905547913801009\n",
      "Log Loss: 6.165049540035453\n",
      "Score: 0.5823016964695094\n",
      "Log Loss: 8.16384010816293\n",
      "Score: 0.5772581384685924\n",
      "Log Loss: 7.222370496676718\n",
      "Score: 0.5955983493810179\n",
      "Log Loss: 6.970619881308975\n",
      "Score: 0.5749656121045392\n",
      "Log Loss: 7.900953884576712\n",
      "Score: 0.5878037597432371\n",
      "Log Loss: 6.896124882530354\n",
      "Score: 0.535992663915635\n",
      "Log Loss: 14.343982550167455\n",
      "Score: 0.5809261806510775\n",
      "Log Loss: 7.238338517190384\n",
      "Score: 0.5635029802842733\n",
      "Log Loss: 10.497512792402302\n",
      "Score: 0.5818431911966988\n",
      "Log Loss: 6.815558184836457\n",
      "Score: 0.5923888124713435\n",
      "Log Loss: 8.233745996440817\n",
      "Score: 0.546079779917469\n",
      "Log Loss: 1.5067524283208238\n",
      "Score: 0.5800091701054562\n",
      "Log Loss: 7.140514544785308\n",
      "Score: 0.5465382851902797\n",
      "Log Loss: 12.267626497172824\n",
      "Score: 0.5887207702888583\n",
      "Log Loss: 8.004305678444334\n",
      "Score: 0.5896377808344796\n",
      "Log Loss: 7.094598217921879\n",
      "Score: 0.5836772122879413\n",
      "Log Loss: 6.347708179008241\n",
      "Score: 0.5749656121045392\n",
      "Log Loss: 7.900953884576712\n",
      "Score: 0.5873452544704264\n",
      "Log Loss: 6.471161647853768\n",
      "Score: 0.5717560751948647\n",
      "Log Loss: 7.414472754824259\n",
      "Score: 0.5910132966529115\n",
      "Log Loss: 6.294758509924242\n",
      "Score: 0.5859697386519945\n",
      "Log Loss: 6.894898960960218\n",
      "Score: 0.5758826226501604\n",
      "Log Loss: 7.568011017414358\n",
      "Score: 0.5868867491976157\n",
      "Log Loss: 7.095855940171286\n",
      "Score: 0.5905547913801009\n",
      "Log Loss: 6.165049540035453\n",
      "Score: 0.5919303071985328\n",
      "Log Loss: 6.919059710620669\n",
      "Score: 0.5492893168271435\n",
      "Log Loss: 10.525060918925504\n",
      "Score: 0.5612104539202201\n",
      "Log Loss: 9.277259931090134\n",
      "Score: 0.5905547913801009\n",
      "Log Loss: 8.176333169806915\n",
      "Score: 0.5974323704722604\n",
      "Log Loss: 6.697293608737011\n",
      "Score: 0.5896377808344796\n",
      "Log Loss: 6.325441245553385\n",
      "Score: 0.5722145804676754\n",
      "Log Loss: 8.069985691510723\n",
      "Score: 0.5896377808344796\n",
      "Log Loss: 6.477410478528527\n",
      "Score: 0.5772581384685924\n",
      "Log Loss: 7.849813971429678\n",
      "Score: 0.5547913801008711\n",
      "Log Loss: 10.221080450729094\n",
      "Score: 0.5483723062815222\n",
      "Log Loss: 12.322192712208231\n",
      "Score: 0.5488308115543329\n",
      "Log Loss: 10.984892269534734\n",
      "Score: 0.5882622650160477\n",
      "Log Loss: 6.390575811026361\n",
      "Score: 0.584135717560752\n",
      "Log Loss: 7.824602175887885\n",
      "Score: 0.569005043558001\n",
      "Log Loss: 2.4479772437979013\n",
      "Score: 0.594222833562586\n",
      "Log Loss: 6.768072809052289\n",
      "Score: 0.5855112333791839\n",
      "Log Loss: 6.894305688192325\n",
      "Score: 0.5832187070151307\n",
      "Log Loss: 7.881151601805604\n",
      "Score: 0.5928473177441541\n",
      "Log Loss: 6.799607455928016\n",
      "Score: 0.6070609812012838\n",
      "Log Loss: 6.37356425305794\n",
      "Score: 0.546079779917469\n",
      "Log Loss: 1.5067524283208238\n",
      "Score: 0.5772581384685924\n",
      "Log Loss: 7.2728415348332165\n",
      "Score: 0.5735900962861072\n",
      "Log Loss: 7.517964743218168\n",
      "Score: 0.5891792755616689\n",
      "Log Loss: 7.7091133147553235\n",
      "Score: 0.5694635488308115\n",
      "Log Loss: 7.408096973621965\n",
      "Score: 0.5859697386519945\n",
      "Log Loss: 6.894898960960218\n",
      "Score: 0.5809261806510775\n",
      "Log Loss: 6.97455817237769\n",
      "Score: 0.5855112333791839\n",
      "Log Loss: 7.315099885269148\n",
      "Score: 0.5749656121045392\n",
      "Log Loss: 7.618686277847016\n",
      "Score: 0.5878037597432371\n",
      "Log Loss: 6.896124882530354\n",
      "Score: 0.594222833562586\n",
      "Log Loss: 5.998628089205933\n",
      "Score: 0.5382851902796882\n",
      "Log Loss: 13.460490644721867\n",
      "Score: 0.5859697386519945\n",
      "Log Loss: 7.740109199496314\n",
      "LOSS 4.804973774247122\n",
      "SCORE 0.5300320953690968\n",
      "VALUE (6,)\n"
     ]
    }
   ],
   "source": [
    "#Analysis of the input data\n",
    "# ...\n",
    "\n",
    "mean = int(np.mean([train_set.shape[1], 10]))\n",
    "# TEST MLC\n",
    "data = np.random.randint(train_set.shape[1], size=100)\n",
    "data = np.hstack([data, mean])\n",
    "best_loss = 100\n",
    "best_score = 100\n",
    "best_val = 0\n",
    "for i in data:\n",
    "    tup = (i, )\n",
    "    clf = train_clf_with_params(1e-5, tup, 1, train_set, train_labels)\n",
    "    score, loss = test_clf(clf, confirm_set, confirm_labels)\n",
    "    if ((score < best_score) & (best_loss > loss)):\n",
    "        best_loss = loss\n",
    "        best_score = score\n",
    "        best_val = tup\n",
    "print(\"LOSS \" + str(best_loss))\n",
    "print(\"SCORE \" + str(best_score))\n",
    "print(\"VALUE \" + str(best_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methods and experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*- Explain your whole approach (you can include a block diagram showing the steps in your process).* \n",
    "\n",
    "*- What methods/algorithms, why were the methods chosen. *\n",
    "\n",
    "*- What evaluation methodology (cross CV, etc.).*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Trials with ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Summarize the results of the experiments without discussing their implications.*\n",
    "\n",
    "*- Include both performance measures (accuracy and LogLoss).*\n",
    "\n",
    "*- How does it perform on kaggle compared to the train data.*\n",
    "\n",
    "*- Include a confusion matrix.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Confusion matrix ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discussion/Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Interpret and explain your results *\n",
    "\n",
    "*- Discuss the relevance of the performance measures (accuracy and LogLoss) for\n",
    "imbalanced multiclass datasets. *\n",
    "\n",
    "*- How the results relate to the literature. *\n",
    "\n",
    "*- Suggestions for future research/improvement. *\n",
    "\n",
    "*- Did the study answer your questions? *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*List of all the references cited in the document*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "*Any additional material needed to complete the report can be included here. For example, if you want to keep  additional source code, additional images or plots, mathematical derivations, etc. The content should be relevant to the report and should help explain or visualize something mentioned earlier. **You can remove the whole Appendix section if there is no need for it.** *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
